name: Daily Player Form Scraper

on:
  schedule:
    - cron: '30 0 * * *' # Runs at 00:30 UTC every day (matches data page text)
  workflow_dispatch: # Allows manual triggering

jobs:
  scrape-form:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Google Chrome and Xvfb
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable xvfb

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run Scraper
        env:
          SUPABASE_PROJECT_URL: ${{ secrets.SUPABASE_PROJECT_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          # Use xvfb-run to provide a virtual display for undetected-chromedriver
          xvfb-run --auto-servernum --server-args="-screen 0 1280x1024x24" python scraper/form.py
